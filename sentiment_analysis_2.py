# -*- coding: utf-8 -*-
"""Sentiment analysis 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12lGQuibeRtOAdbcC54vtQQUWuFW3UAkW
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow import keras
from tensorflow.keras.utils import to_categorical

data = pd.read_csv(r"C:\Users\Anjali\Downloads\sentiment_analysis.csv")# Replace with your dataset file
texts = data['text']  # Replace 'content' with your text column name
labels = data['sentiment']  # Replace 'sentiment' with your labels column name

# Map string labels to numeric values
label_mapping = {
    'positive': 1,
    'negative': 0,
    'neutral': 2  # Include all classes in your dataset
}
labels = labels.map(label_mapping)

train_sentences, test_sentences, train_labels, test_labels = train_test_split(
    texts, labels, stratify=labels, test_size=0.2, random_state=42
)

vocab_size = 3000
embedding_dim = 100
max_length = 200
oov_tok = '<OOV>'

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(train_sentences)

train_sequences = tokenizer.texts_to_sequences(train_sentences)
train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')

test_sequences = tokenizer.texts_to_sequences(test_sentences)
test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')

model = keras.Sequential([
    keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),
    keras.layers.Bidirectional(keras.layers.LSTM(128)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(3, activation='softmax')  # 3 classes: positive, negative, neutral
])

model.compile(
    loss='sparse_categorical_crossentropy',  # For integer-encoded labels
    optimizer='adam',
    metrics=['accuracy']
)

history = model.fit(
    train_padded, np.array(train_labels),
    validation_split=0.1,
    epochs=10,
    batch_size=32,
    verbose=1
)

model.evaluate(test_padded, np.array(test_labels))

new_text = "This app is fantastic! I love using it every day!"
sequence = tokenizer.texts_to_sequences([new_text])
padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')
prediction = model.predict(padded_sequence)

classes = ['negative', 'positive', 'neutral']
predicted_class = np.argmax(prediction)

classes[predicted_class]

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import layers, models, regularizers
from tensorflow.keras.callbacks import EarlyStopping

# Step 1: Load the dataset
data = pd.read_csv(r"C:\\Users\\Anjali\\Downloads\\sentiment_analysis.csv")
texts = data['text']  # Replace 'text' with the actual column name containing text
labels = data['sentiment']  # Replace 'sentiment' with the actual column name for labels

# Step 2: Encode labels into integers
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(labels)  # Converts 'positive', 'neutral', 'negative' to integers

# Step 3: Split data into training and test sets
train_sentences, test_sentences, train_labels, test_labels = train_test_split(
    texts, labels, stratify=labels, test_size=0.2, random_state=42
)

# Step 4: Tokenize and pad the sequences
vocab_size = 3000
embedding_dim = 100
max_length = 200
oov_tok = '<OOV>'

tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(train_sentences)

train_sequences = tokenizer.texts_to_sequences(train_sentences)
test_sequences = tokenizer.texts_to_sequences(test_sentences)

train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')
test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')

# Step 5: Build the model with stronger regularization
model = models.Sequential([
    layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),
    layers.SpatialDropout1D(0.3),  # Higher dropout rate
    layers.Conv1D(64, 5, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  # L2 regularization
    layers.MaxPooling1D(pool_size=2),
    layers.Conv1D(32, 3, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  # Smaller filter size
    layers.GlobalMaxPooling1D(),
    layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  # L2 regularization
    layers.Dropout(0.5),  # Higher dropout rate in dense layers
    layers.Dense(3, activation='softmax')  # Assuming 3 sentiment classes
])

# Step 6: Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Step 7: Train the model with early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model.fit(
    train_padded, train_labels,
    validation_split=0.2,
    epochs=30,  # Train longer with patience for validation improvement
    batch_size=64,  # Use a larger batch size for better generalization
    callbacks=[early_stopping]
)

# Step 8: Evaluate the model
test_loss, test_accuracy = model.evaluate(test_padded, test_labels)
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")

# Step 9: Test with a new text
new_text = ["This product is amazing and exceeded my expectations!"]
new_sequence = tokenizer.texts_to_sequences(new_text)
new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')

prediction = model.predict(new_padded)
predicted_class = np.argmax(prediction)
classes = label_encoder.classes_  # Decode class indices back to labels
print(f"Predicted Sentiment: {classes[predicted_class]}")

new_text = ["I hate myself beacuse i am ugly..."]
new_sequence = tokenizer.texts_to_sequences(new_text)
new_padded = pad_sequences(new_sequence, maxlen=max_length, padding='post', truncating='post')

prediction = model.predict(new_padded)
predicted_class = np.argmax(prediction)
classes = label_encoder.classes_  # Decode class indices back to labels
print(f"Predicted Sentiment: {classes[predicted_class]}")

